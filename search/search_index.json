{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Performance benchmarking study of Open Source 5G Core networks","text":"<p>This gives a tutorial on the performance study of open source 5g networks. We use this 5G traffic generator to generate and perform registration procedures for multiple UEs. We will use bcc tools and bpftrace tools to do a granular performance analysis of the underlying resources whilst load testing the 5G core networks.</p> <p>We look at the following 5G core networks</p> <ol> <li>Open Air Interface - 5G CORE NETWORK</li> <li>free5gc</li> <li>Open5gs</li> </ol> <p>In the study we configure the 5G core networks to use a single and the same ciphering and encryption NIA1 and NEA1.</p> <p>We set up the environment on openstack. In our case we install openstack locally on our workstation. We then set up the traffic generator and the core networks on VM on openstack and configure for communication. To make it easier to set up and run the performance analysis we make use of Ansible. Ansible helps in making the study easy to reproduce and the results easier to collect. We author the ansible roles and plays necessary for this study on this repository.</p> <p>This study will:</p> <ol> <li>Install and set up openstack on a workstation</li> <li>Set up and install the traffic generator on a VM on openstack</li> <li>Detail how to set up performance analysis tools</li> <li>Install, set up OAI CN and collect performance analysis logs</li> <li>Install, set up free5gc and collect performance analysis logs</li> <li>Install, set up Open5gs and collect performance analysis logs</li> </ol>"},{"location":"#install-and-set-up-openstack-on-a-workstation","title":"Install and set up openstack on a workstation","text":"<p>There are two options for setting up the local cloud (testbed), using Microstack and using Devstack. Microstack worked fine for a start but there we couple of issues I had to workaround. You can see this under the Microstack Gotchas section. One of the issues ended up reoccuring and could resolve it so had to switch to Devstack. It is possible that one might not face the issue on their environment. Based on this we recommend using Devstack for replicaing the study. Aside from the issues encountered with Microstack, with Devstack you can use the latest stable version of openstack <code>zed</code> where as Microstack will install <code>ussuri</code> (at the time of writing).</p> <p>Installing devstack</p> <pre><code># Add user\nsudo useradd -s /bin/bash -d /opt/stack -m stack\n\n# \nsudo chmod +x /opt/stack\n\n#\necho \"stack ALL=(ALL) NOPASSWD: ALL\" | sudo tee /etc/sudoers.d/stack\nsudo -u stack -i\n\ngit clone https://opendev.org/openstack/devstack\ncd devstack\n\n# Switch to openstack version of choice\ngit checkout stable/zed\n</code></pre> <p>Configure credentials</p> <p>Need to create credentials config file (<code>local.conf</code>) before installing the stack inside folder devstack. See example below.</p> <p>Note: putting HOST_IP=0.0.0.0 will ensure that openstack doesn't bind to you network interface IP address. This is helpful when you are on WIFI and you IP is dynamically allocated or changes depending on the network</p> <pre><code>[[local|localrc]]\nADMIN_PASSWORD=secret\nDATABASE_PASSWORD=$ADMIN_PASSWORD\nRABBIT_PASSWORD=$ADMIN_PASSWORD\nSERVICE_PASSWORD=$ADMIN_PASSWORD\nHOST_IP=0.0.0.0\n</code></pre> <p>Note: When installing on ubuntu 22.04 got an error of repository/PPA does not have a Release file for some of the packages. Implemented the workaround from here, which is to use PPA release files for ubuntu 20.04.</p> <pre><code># Change PPA configuration\nsudo sed -i 's/jammy/focal/g' /etc/apt/sources.list.d/gezakovacs-ubuntu-ppa-jammy.list\nsudo sed -i 's/jammy/focal/g' /etc/apt/sources.list.d/system76-ubuntu-pop-jammy.list\n</code></pre> <p>Install Devstack</p> <pre><code>./stack.sh\n</code></pre> <p>When you restart the service you might have issues with devstack. In my case I had error with openvswitch. Resolved it by following steps on this StackOverflow thread.</p> <p>Using openstack CLI</p> <p>In order to use the CLI you will need to set the env variables.</p> <pre><code>sudo su - stack\ncd devstack\n\n# username: admin, project: demo\nsource openrc admin demo\n</code></pre> <p>Download and create Ubuntu image</p> <pre><code>cd ~/\nmkdir images\n\n# Download image\nwget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img -o images/focal-server-cloudimg-amd64.img\n\n# Create Ubuntu image\nopenstack image create \\\n    --container-format bare \\\n    --disk-format qcow2 \\\n    --min-disk 8 --min-ram 512 \\\n    --file images/focal-server-cloudimg-amd64.img \\\n    --public 20.04\n\n# Confirm image created\nopenstack image list\n\n# Create flavor we are using for testing\nopenstack flavor create --public m2.medium --id auto \\\n    --ram 4096 --disk 50 --vcpus 2 --rxtx-factor 1\n</code></pre> <p>Create ssh keys to attach to servers</p> <pre><code># Generate keys\nssh-keygen -t rsa -b 4096\n\n# Add key to openstack\nopenstack keypair create --public-key /opt/stack/.ssh/id_rsa.pub stack\n\n# Confirm key was created\nopenstack keypair list\n</code></pre> <p>Setup the rules to enable networking of the VMs with the internet</p> <pre><code># On HOST machine: Enable traffic to be correctly routed out of the VMs on Devstack\necho 1 &gt; /proc/sys/net/ipv4/ip_forward\necho 1 &gt; /proc/sys/net/ipv4/conf/&lt;interface&gt;/proxy_arp\niptables -t nat -A POSTROUTING -o &lt;interface&gt; -j MASQUERADE\n\n# Devstack does not wire up the public network by default so we must do that before connecting to this floating IP address.\nsudo ip link set br-ex up\nsudo ip route add 172.24.4.0/24 dev br-ex\nsudo ip addr add 172.24.4.1/24 dev br-ex\n\n# By default, DevStack does not allow users to access VMs, to enable that, we will need to add a rule. We will allow both ICMP and SSH.\n# If you get error of more than one security group with name default, use the security group id instead\nopenstack security group rule create --ingress --ethertype IPv4 --dst-port 22 --protocol tcp default\nopenstack security group rule create --ingress --ethertype IPv4 --protocol ICMP default\nopenstack security group rule list\n\n# Enable DHCP for the VMs\nopenstack subnet set --dhcp private-subnet\nopenstack subnet set --dns-nameserver 8.8.8.8 private-subnet\n</code></pre> <p>Create servers for testing (Documented instructions from here)</p> <pre><code># Get net id for private network\nPRIVATE_NET_ID=$(openstack network show private -c id -f value)\n\n# Create server (core network)\nopenstack server create --flavor m2.medium \\\n    --image 20.04 \\\n    --key-name  stack \\\n    --nic net-id=${PRIVATE_NET_ID} \\\n    &lt;server-name&gt;\n\nopenstack floating ip create --project demo --subnet public-subnet public\n\nopenstack server add floating ip &lt;server-name&gt; &lt;float-ip&gt;\n# Confirm\nopenstack server list\n\n# Test ping\nping -c 4 &lt;ip-address&gt;\n\n# Confirm SSH into instance\nssh ubuntu@&lt;float-ip&gt;\n</code></pre> <p>Uninstall Devstack</p> <pre><code># Clean\n./clean.sh\n\n# Remove\n./unstack.sh\n</code></pre>"},{"location":"#microstack-gotchas","title":"Microstack Gotchas","text":"<p>Encountered a recurring error <code>Permission denied (publickey)</code>. Initially disabling and re-enabling microstack worked. See thread here. However this doesn't seem to work everytime. It ended up disrupting the study. The details of the issue are described here</p> <p>Microstack during installation binds to the IP address of the primary interface. When restarting the workstation sometimes microstack would become unavailable. You are able to get the login page but ultimately you can't see the dashboard. Running <code>sudo snap logs microstack</code> showed one of the error to be <code>Can't connect to MySQL server on '192.168.100.11' ([Errno 113] No route to host</code>. In general all the error logs had to do with connection. Turns out that microstack hardcoded the external ip address during installation. On an laptop environment, a laptop using wifi and dynamic ip allocation, the external ip address changes on reboot. This is bug is also discussed on here. The resolution was to set the wifi interface to a static ip address. I after this I had to reboot my machine, disable then enable microstack. Maybe one of those steps might not be necessary. These steps resolved my issue.</p>"},{"location":"#how-to-set-up-performance-analysis-tools","title":"How to set up performance analysis tools","text":"<p>To set up the tools you will need to create a VM to run them on. In this study you will need to set up the tools on the VMs for the core networks. The section is to be referenced after setting up the VMs.</p>"},{"location":"#set-up-bcc-tools-to-collect-system-performance-results","title":"Set up bcc tools to collect system performance results","text":"<p>We will install bcc from source because we want to print the results in json format for easier visualisation. We raised a PR for this to be part of the bcc project here, but it's not yet merged, still under consideration. The source is a fork repo.</p> <p>To install we are going to use an ansible role. One downside of the ansisble ad-hoc cli commands that we are using is they don't have access to the ansible_facts which the role. A workaround is to cache them and then use them in the next command. This is recommanded on a comment on this thread. </p> <pre><code># Cache the ansible_facts\nANSIBLE_CACHE_PLUGIN=jsonfile ANSIBLE_CACHE_PLUGIN_CONNECTION=/tmp/ansible-cache \\\nansible all -i '172.24.4.3,' -u ubuntu -m setup\n\n# Run the ansible role for OAI. Replace 172.24.4.3 with the IP of the OAI VM\nANSIBLE_CACHE_PLUGIN=jsonfile ANSIBLE_CACHE_PLUGIN_CONNECTION=/tmp/ansible-cache \\\nansible all -i '172.24.4.3,' -u ubuntu -m include_role --args \"name=bcc\" -e user=ubuntu\n</code></pre>"},{"location":"#set-up-bpftrace-to-collect-system-performance-results","title":"Set up bpftrace to collect system performance results","text":"<p>We will build from source because the version of bpftrace on ubuntu packages doesn't allow printing of output in json format. Json output format is important for graphing the results.</p> <p>We will use an ansible role for this.</p>"},{"location":"#copy-bash-script-to-collect-results-partitioned-by-variables","title":"Copy bash script to collect results partitioned by variables","text":"<pre><code># Copy tools file\nansible all -i '172.24.4.3,' -u ubuntu -m ansible.builtin.copy -a \"src=files/tools dest=/home/ubuntu\"\n\n# Make main script executable\nansible all -i '172.24.4.3,' -u ubuntu -m ansible.builtin.file -a \"dest=/home/ubuntu/tools/main.sh mode=a+x\"\n</code></pre>"},{"location":"#set-up-and-install-the-traffic-generator-on-a-vm-on-openstack","title":"Set up and install the traffic generator on a VM on openstack","text":"<p>Start by creating a VM for the traffic generator as detailed in the earlier section. Now ssh into the VM and run the commands below.</p> <pre><code>sudo apt update\n\n# Download the traffic generator\ngit clone https://github.com/tariromukute/core-tg.git\n\n# Install dependecies\ncd core-tg/\ngit submodule init\ngit submodule update\n\nsudo apt-get install python3-dev\nsudo apt-get install build-essential\nsudo apt install python3.8-venv\n\n# Create virtual environment for the app and install requirements\npython3 -m venv .venv\nsource .venv/bin/activate\n\npip install pycrate\npip install pysctp\npip install cryptography\npip install pyyaml\n\n# Set up the CryptoMobile module\ncd CryptoMobile &amp;&amp; python3 setup.py install\n</code></pre>"},{"location":"#install-set-up-oai-cn-and-collect-performance-analysis-logs","title":"Install, set up OAI CN and collect performance analysis logs","text":"<p>We can set up OAI by running the steps from the GitLab repository. However, created an ansible role that can set up OAI. The ansible role should make it easier. If you prefere you can create the VM and install following the instructions from OAI repository.</p> <p>Create a VM for OAI as describe under in previous section</p> <p>Set up using an ansible role</p> <p>To set up OAI you can follow the instruction on the offical site. Created an ansible role that can set up OAI. The role does the following. 1. Install dependencies for OAI 2. Pulls and runs the OAI docker images 3. Sets up the networking rules to allow traffic forward on VM to the docker containers 4. Add an sql dump to initialise OAI with UEs for testing 208950000000031 - 208950000100031 5. Copies docker-compose file to run OAI on network 48.0.0.0/16.</p> <pre><code># Run the ansible role for OAI. Replace 172.24.4.223 with the IP of the OAI VM\n\n# Cache the ansible_facts\nANSIBLE_CACHE_PLUGIN=jsonfile ANSIBLE_CACHE_PLUGIN_CONNECTION=/tmp/ansible-cache \\\nansible all -i '172.24.4.223,' -u ubuntu -m setup\n\nANSIBLE_CACHE_PLUGIN=jsonfile ANSIBLE_CACHE_PLUGIN_CONNECTION=/tmp/ansible-cache \\\nansible all -i '172.24.4.223,' -u ubuntu -m include_role --args \"name=oai_cn5g\" -e user=ubuntu\n</code></pre> <p>Start the 5G Core</p> <pre><code>systemctl restart oai-cn5g-*\n</code></pre> <p>Start the 5G core traffic generator</p> <pre><code>cd ~/core-tg/\nsource .venv/bin/activate\n\npython3 run.py -u config/oai-cn5g-ue.yaml -g config/oai-cn5g-gnb.yaml -vvv\n</code></pre>"},{"location":"#collect-performance-analysis-results","title":"Collect performance analysis results","text":"<p>First you need to install the tools for performance analysis as explaned in this section. We will make use of an ansible play on this repo to collect the results. The play will: 1. Restart the core network 2. Start the specified performance analysis tool 3. Start the traffic generator 4. After the specified duration pull the performance results and traffic generator logs 5. Save the logs in the <code>.results folder</code></p> <p>Before running you will need to create an inventory file <code>inventory.ini</code> for the VMs. Paste the contents below in the file. client1 should be the ip of the traffic generator and server1 should be the ip of the core network.</p> <pre><code>[az_trex]\nclient1 ansible_host=172.24.4.49 ansible_user=ubuntu ansible_ssh_private_key_file=/opt/stack/.ssh/id_rsa\n[az_sut]\nserver1 ansible_host=172.24.4.3 ansible_user=ubuntu ansible_ssh_private_key_file=/opt/stack/.ssh/id_rsa\n</code></pre> <p>Afterwards run the play to collect logs.</p> <pre><code>cd ansible\n\n# Start Core Network and Traffic generator\nansible all -i inventory.ini -u ubuntu -m include_tasks -a file=plays/oai.yml \\\n    -e user=ubuntu -e duration=20 -e aduration=35 -e interval=0 \\\n    -e tool=syscount -e ues=50\n</code></pre> <p>Visualise the results by using this notebook </p>"},{"location":"#oai-gotchas","title":"OAI Gotchas","text":"<p>Tried running the OAI on Ubuntu 20.04 VM on microstack. The oai-amf container failed with socket error. Realised that this was due to the SCTP module missing on the kernel <code>lsmod | grep sctp</code>. I tried locating the module with <code>modinfo sctp</code> but it was not found. I ran <code>sudo apt install linux-generic</code> to get the extra modules. I could now find the module and tried loading with <code>insmod &lt;path_to_module&gt;</code>. This failed. Turns out I was using the <code>focal-server-cloudimg-amd64-disk-kvm.img</code> as recommended or pointed to on one of the Microstack blogs. I switched to creating a VM from image <code>focal-server-cloudimg-amd64.img</code>. This also didn't have the SCTP module load but I could find it on the system. I loaded the module <code>modprobe sctp</code> and then ran the OAI and this time it worked. I assume this would be the case for all the core networks. Used the same image for the rest of the core networks.</p> <p>The OAI CN generates a lot of debug logs. Although the documentation (at the time of writing) states that the network functions produce info level logs see docs. The docker containers from oaisoftwarealliance tags v1.4.0 and v1.5.0 produce debug logs. When doing load testing this affects the performance of the core network.</p>"},{"location":"#install-set-up-free5gc-and-collect-performance-analysis-logs","title":"Install, set up free5gc and collect performance analysis logs","text":"<p>Start by creating a VM for OAI as describe under in previous section</p> <p>We can set up freegc by following the instruction from the free5gc repository. We created an ansible role that can set up free5gc. The ansible role should make it easier. If you prefere you can create the VM and install following the instructions from free5gc repository.</p> <p>Install Free5gc</p> <pre><code># Run the ansible role for free5gc. Replace 172.24.4.223 with the IP of the free5gc VM\n\n# Cache the ansible_facts\nANSIBLE_CACHE_PLUGIN=jsonfile ANSIBLE_CACHE_PLUGIN_CONNECTION=/tmp/ansible-cache \\\nansible all -i '172.24.4.223,' -u ubuntu -m setup\n\nANSIBLE_CACHE_PLUGIN=jsonfile ANSIBLE_CACHE_PLUGIN_CONNECTION=/tmp/ansible-cache \\\nansible all -i '172.24.4.223,' -u ubuntu -m include_role --args \"name=free5gc\" -e user=ubuntu\n</code></pre> <p>Start the 5G Core</p> <pre><code>systemctl restart free5gc\n# the anisble role creates a unit service for free5gc. If you set up from the repo, run below commands\ncd ~/free5gc\n./run.sh\n</code></pre> <p>Start the 5G core traffic generator</p> <p>You will need to update the ip address in the files <code>config/free5gc-ue.yaml</code> and  <code>config/free5gc-gnb.yaml</code> on the core network VM.</p> <pre><code>cd ~/core-tg/\nsource .venv/bin/activate\n\npython3 run.py -u config/free5gc-ue.yaml -g config/free5gc-gnb.yaml -vvv\n</code></pre> <p>Based on the logs you can check if the traffic is flowing and there has been registration.</p> <ol> <li>Check the OAI logs for each service <code>journalctl -u free5gc -n 200</code></li> </ol> <p>Collect performance analysis results</p> <p>To setup for collection the tools follow instructions here, to start the collection run the commands below. Notice that we use a different play from the same repository here</p> <pre><code>cd ansible\n\n# Start Core Network and Traffic generator\nansible all -i inventory.ini -u ubuntu -m include_tasks -a file=plays/free5gc.yml \\\n    -e user=ubuntu -e duration=20 -e aduration=35 -e interval=0 \\\n    -e tool=syscount -e ues=50\n</code></pre> <p>Visualise the results by using this notebook </p>"},{"location":"#install-set-up-open5gs-and-collect-performance-analysis-logs","title":"Install, set up Open5gs and collect performance analysis logs","text":"<p>Start by creating a VM for OAI as describe under in previous section</p> <p>We can set up freegc by following the instruction from the open5gs installation guide. We created an ansible role that can set up open5gs. The ansible role should make it easier. If you prefere you can create the VM and install following the instructions from open5gs installation guide.</p> <p>Install Open5gs</p> <pre><code># Firstly install ansible\n\n# Replace 172.24.4.163 with the IP of the Open5gs VM\n# Cache the ansible_facts\nANSIBLE_CACHE_PLUGIN=jsonfile ANSIBLE_CACHE_PLUGIN_CONNECTION=/tmp/ansible-cache \\\nansible all -i '172.24.4.163,' -u ubuntu -m setup\n\nANSIBLE_CACHE_PLUGIN=jsonfile ANSIBLE_CACHE_PLUGIN_CONNECTION=/tmp/ansible-cache \\\nansible all -i '172.24.4.163,' -u ubuntu -m include_role --args \"name=open5gs\" -e user=ubuntu\n</code></pre> <p>Start the 5G Core</p> <pre><code># systemctl restart open5gs-* had issues because systemctl restart open5gs-dbctl was missing\nsystemctl restart open5gs-amfd open5gs-upfd open5gs-scpd open5gs-nrfd open5gs-mmed open5gs-udrd open5gs-sgwud open5gs-sgwcd open5gs-ausfd open5gs-pcrfd open5gs-pcfd open5gs-bsfd open5gs-hssd open5gs-nssfd open5gs-udmd open5gs-smfd\n</code></pre> <p>Start the 5G core traffic generator</p> <p>You will need to update the ip address in the files <code>config/open5gs-ue.yaml</code> and  <code>config/open5gs-gnb.yaml</code> on the core network VM.</p> <pre><code>cd ~/core-tg/\nsource .venv/bin/activate\n\npython3 run.py -u config/open5gs-ue.yaml -g config/open5gs-gnb.yaml -vvv\n</code></pre> <p>Based on the logs you can check if the traffic is flowing and there has been registration.</p> <ol> <li>Check the OAI logs for each service <code>journalctl -u open5gs-* -n 200</code></li> </ol> <p>Collect performance analysis results</p> <p>To setup for collection the tools follow instructions here, to start the collection run the commands below. Notice that we use a different play from the same repository here</p> <pre><code>cd ansible\n\n# Start Core Network and Traffic generator\nansible all -i inventory.ini -u ubuntu -m include_tasks -a file=plays/open5gs.yml \\\n    -e user=ubuntu -e duration=20 -e aduration=35 -e interval=0 \\\n    -e tool=syscount -e ues=50\n</code></pre>"},{"location":"blog/","title":"Performane evaluation of open-source 5G Core networks","text":""},{"location":"blog/control-operations/","title":"Control operations syscalls","text":"<p>The sched_yield system call is used by a thread to allow other threads a chance to run, and \\ the calling thread relinquishes the CPU. Strategic calls to sched_yield() can improve performance by giving \\ other threads or processes an opportunity to run when (heavily) contended resources, such as mutexes, have been \\ released by the caller. The authors of were able to improve the throughput of their system by employing \\ the sched_yield system call after a process processes each batch of packets before calling the poll. On the other \\ hand, sched_yield can result in unnecessary context switches, which will degrade system performance if not used \\ appropriately. The latter is mainly true in generic Linux systems, as the scheduler is responsible for deciding \\ which process runs. In most cases, when a process yields, the scheduler may perceive it as a higher priority and \\ still put it back into execution, where it yields again in a loop. This behaviour is mainly due to the algorithm and \\ logic used by Linux\u2019s default scheduler to determine the process with the higher prior</p>"},{"location":"blog/files-operations/","title":"File operations syscalls","text":""},{"location":"blog/files-operations/#readwrite","title":"read/write","text":"<p>The read() system call is used to retrieve data from a file stored in the file system, while the write() system call is used to write data from a buffer to a file. Both system calls take into account the \"count\", which represents the number of bytes to read or write. Upon successful execution, these system calls return the number of bytes that were successfully read or written. By default, these system calls are blocking but can be changed to non-blocking using the fnctl system call. Blocking is a problem for programs that should operate concurrently, since blocked processes are suspended. There are two different, complementary ways to solve this problem. They are nonblocking mode and I/O multiplexing system calls, such as select and epoll. The architectural decision to use a combination of multiplexing I/O operations and non-blocking system calls offers advantages depending on the use cases. Some scenarios where this approach is beneficial include situations where small buffers would result in repeated system calls, when the system is dedicated to one function, or when multiple I/O system calls return an error.</p>"},{"location":"blog/io-multiplexing/","title":"I/O Multiplexing syscalls","text":"<p>The system calls epoll/poll/select implement I/O multiplexing, which enables the simultaneous monitoring of multiple input and output sources in a single operation. These system calls are based on the Linux design principle, which considers everything as a file and operates by monitoring files to determine if they are ready for the requested operation. The main advantage of multiplexing I/O operations is that it avoids blocking read and write where a process will wait for data while on the CPU. Instead, one waits for the multiplexing I/O system calls to determine which files are ready for read or write.</p>"},{"location":"blog/locks-operations/","title":"Locks operations syscalls","text":""},{"location":"blog/locks-operations/#futex","title":"futex","text":"<p>The futex() system call offers a mechanism to wait until a specific condition becomes true. It is typically used as a blocking construct in the context of shared-memory synchronisation. Additionally, futex() operations can be employed to wake up processes or threads that are waiting for a particular condition. The main design goal of futex is to manage the mutex keys in the user space to avoid context switches when handling mutex in kernel space. In the futex design, the kernel is involved only when a thread needs to sleep or the system needs to wake up another thread. Essentially, the futex system call can be described as providing a kernel side wait queue indexed by a user space address, allowing threads to be added or removed from user space. A high frequency of calls to the futex system may indicate a high degree of concurrent access to shared resources or data structures by multiple threads or processes.</p>"},{"location":"blog/overall/","title":"Overal performance","text":"<p>This section presents the high-level performance of the three 5GCs under evaluation. We present the time taken to complete the registration and de-registration procedures for a given number of UEs and the average time it takes to complete registration and de-registration of a single UE. In both cases, we show the results as the UE load increases. Open5GS leads in performance, with free5GC coming next and OAI trailing behind.</p>"},{"location":"blog/process-across-system/","title":"Processes making syscalls","text":"<p>The information about the processes that make system calls provides valuable insights into the most active processes during the registration procedure. By observing the changes in latency and frequency of the system calls made by a process as the number of UEs increases, we can identify processes that have a high probability of becoming bottlenecks. The information can be used to make several mitigation decisions, such as allocating more resources or dedicated resources to a given process or Network Function (NF), optimising the usage by the NF or process, and examining the configuration of the process, among other things.</p>"},{"location":"blog/sleep-operations/","title":"Sleep operations syscalls","text":""},{"location":"blog/sleep-operations/#nanosleepclock_nanosleep","title":"nanosleep/clock_nanosleep","text":"<p>The nanosleep and clock_nanosleep system calls are used to allow the calling thread to sleep for a specific interval with nanosecond precision. The clock_nanosleep differs from nanosleep in two ways. Firstly, it allows the caller to select the clock against which the sleep interval is to be measured. Secondly, it enables the specification of the sleep interval as either an absolute or a relative value. Using an absolute timer is useful to prevent timer drift issues mentioned about nanosleep.</p>"},{"location":"blog/socket-read-operations/","title":"Socket read syscalls","text":""},{"location":"blog/socket-read-operations/#recv-recvfrom-recvmsg-recvmmsg-recvfrom-recvmsg-and-recvmmsg","title":"recv, recvfrom, recvmsg, recvmmsg. recvfrom(), recvmsg() and recvmmsg()","text":"<p>These are all system calls used to receive messages from a socket. They can be used to receive data on a socket, whether or not it is connection-orientated. These system calls are blocking calls; if no messages are available at the socket, the receive calls wait for a message to arrive. If the socket is set to non-blocking, then the value -1 is returned and errno is set to EAGAIN or EWOULDBLOCK. Passing the flag MSG_DONTWAIT to the system call enables non-blocking operation. This provides behaviour similar to setting O_NONBLOCK with fcntl except MSG_DONTWAIT is per operation. The recv() call is normally used only on a connected socket and is identical to recvfrom() with a nil from parameter. recv(), recvfrom() and recvmsg() calls return the number of bytes received, or -1 if an error occurred. For connected sockets whose remote peer was shut down, 0 is returned when no more data is available. The recvmmsg() call returns the number of messages received, or -1 if an error occurred.</p>"},{"location":"blog/socket-write-operations/","title":"Socket write syscalls","text":""},{"location":"blog/socket-write-operations/#send-sendto-sendmsg-sendmmsg","title":"send, sendto, sendmsg, sendmmsg","text":"<p>The send() call may only be used when the socket is in a connected state (so that the intended recipient is known). The send() is similar to write() with the difference of flags. The sendto and sendmsg work on both connected and unconnected sockets. The sendmsg() call also allows sending ancillary data (also known as control information). The sendmmsg() system call is an extension of sendmsg that allows the caller to transmit multiple messages on a socket using a single system call. The approaches to optimise the send(s) system calls are similar to the discussed approaches for the recv(s) system calls. These include I/O multiplexing, using the system calls in non-blocking mode, and sending multiple messages in a single system call where possible.</p>"},{"location":"blog/syscalls-across-system/","title":"Syscalls ascross the system","text":"<p>Analysing system calls (syscalls) across the system helps in categorising the workload of the system. This information is valuable in identifying the hardware resources that require optimisation, such as installing an accelerated network card interface (NIC) or a cryptographic accelerator.</p>"},{"location":"blog/2023/12/31/processes-making-syscalls/","title":"Processes making syscalls","text":"<p>We hope you are all having fun and wish you all the best for the new year!</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>"},{"location":"blog/2023/12/31/syscalls-ascross-the-system/","title":"Syscalls ascross the system","text":"<p>We hope you are all having fun and wish you all the best for the new year!</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>"},{"location":"blog/archive/2023/","title":"2023","text":""}]}